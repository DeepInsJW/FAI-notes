{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the paper: https://arxiv.org/pdf/1708.02002.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from matplotlib import patches, patheffects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_path = Path('/scratch/arka/Ark_git_files/coco/')\n",
    "ann_path = coco_path / 'annotations'\n",
    "train_path = coco_path / 'train2017'\n",
    "val_path = coco_path / 'val2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {ann_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train2017 = json.load((ann_path / 'captions_train2017.json').open('r'))\n",
    "captions_val2017 = json.load((ann_path / 'captions_val2017.json').open('r'))\n",
    "instances_train2017 = json.load((ann_path / 'instances_train2017.json').open('r'))\n",
    "instances_val2017 = json.load((ann_path / 'instances_val2017.json').open('r'))\n",
    "person_keypoints_train2017 = json.load((ann_path / 'person_keypoints_train2017.json').open('r'))\n",
    "person_keypoints_val2017 = json.load((ann_path / 'person_keypoints_val2017.json').open('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train2017.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{:012}'.format(instances_train2017['annotations'][0]['image_id']) + '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list = []\n",
    "# print(instances_train2017['annotations'][0]['image_id'])\n",
    "# instances_train2017['annotations'][0]['id']\n",
    "for d in tqdm(instances_train2017['annotations']):\n",
    "    iid = d['id']\n",
    "    image_id = d['image_id']\n",
    "    bbox = d['bbox']\n",
    "    cat_id = d['category_id']\n",
    "    train_df_list.append([iid, image_id, bbox, cat_id])\n",
    "train_df = pd.DataFrame(train_df_list, columns=['id','image_id', 'bbox', 'category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_list = []\n",
    "# print(instances_train2017['annotations'][0]['image_id'])\n",
    "# instances_train2017['annotations'][0]['id']\n",
    "for d in tqdm(instances_val2017['annotations']):\n",
    "    iid = d['id']\n",
    "    image_id = d['image_id']\n",
    "    bbox = d['bbox']\n",
    "    cat_id = d['category_id']\n",
    "    val_df_list.append([iid, image_id, bbox, cat_id])\n",
    "val_df = pd.DataFrame(val_df_list, columns=['id','image_id', 'bbox', 'category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "CSVS = coco_path / 'CSVS'\n",
    "CSVS.mkdir(exist_ok=True)\n",
    "\n",
    "train_csv = CSVS / 'train.csv'\n",
    "val_csv = CSVS / 'val.csv'\n",
    "\n",
    "category_mapping = coco_path / 'category_map.json'\n",
    "inv_category_mapping = coco_path / 'inv_category_map.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(train_csv, header=True, index=False)\n",
    "val_df.to_csv(val_csv, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = dict()\n",
    "inv_cmap = dict()\n",
    "for i in instances_train2017['categories']:\n",
    "    cmap[i['id']] = i['name']\n",
    "    inv_cmap[i['name']] = i['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(cmap, category_mapping.open('w'))\n",
    "json.dump(inv_cmap, inv_category_mapping.open('w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv, header='infer')\n",
    "val_df = pd.read_csv(val_csv, header='infer')\n",
    "cat_map = json.load(category_mapping.open('r'))\n",
    "inv_cat_map = json.load(inv_category_mapping.open('r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catlist = []\n",
    "cat2lab = dict()\n",
    "lab2cat = dict()\n",
    "c = 0\n",
    "for i, v in cat_map.items():\n",
    "    catlist.append(int(i))\n",
    "    cat2lab[int(i)] = c\n",
    "    lab2cat[c] = int(i)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw_bb(bb): return np.array([bb[1], bb[0], bb[3]+bb[1]-1, bb[2]+bb[0]-1])\n",
    "def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "class CocoDS(BaseDataset):\n",
    "    def __init__(self, df, tdir, sz=224, tfm1=None):\n",
    "        # ['id', 'image_id', 'bbox', 'category_id']\n",
    "        self.df = df\n",
    "        self.tdir = tdir        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.sz = sz\n",
    "        # tfm1 for bbox\n",
    "        self.tfm1 = tfm1\n",
    "        \n",
    "    def get_n(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_c(self):\n",
    "        return 80\n",
    "    \n",
    "    def get_sz(self):\n",
    "        return self.sz\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        fid = '{:012}'.format(self.df.iloc[i]['image_id']) + '.jpg'\n",
    "        fname = self.tdir / fid\n",
    "        return open_image(fname)\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        y_bbox = self.df.iloc[i]['bbox']\n",
    "        cat_id = self.df.iloc[i]['category_id']\n",
    "        y_id = cat2lab[cat_id]\n",
    "        y_bbox = ast.literal_eval(y_bbox)\n",
    "        return hw_bb(np.array(y_bbox)), y_id\n",
    "    \n",
    "    def get1item(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        x, y1 = self.tfm1(x, y[0])\n",
    "        return x, (y1, y[1])\n",
    "    \n",
    "    def denorm(self,arr):\n",
    "        \"\"\"Reverse the normalization done to a batch of images.\n",
    "        Arguments:\n",
    "            arr: of shape/size (N,3,sz,sz)\n",
    "        \"\"\"\n",
    "        if type(arr) is not np.ndarray: arr = to_np(arr)\n",
    "        if len(arr.shape)==3: arr = arr[None]\n",
    "        return self.tfm1.denorm(np.rollaxis(arr,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = A([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "sz = 224\n",
    "# tfms = tfms_from_stats(imagenet_stats, sz, aug_tfms=transforms_basic, tfm_y=TfmType.COORD)\n",
    "bs = 9\n",
    "augs = [RandomFlip(tfm_y=TfmType.COORD),\n",
    "        RandomRotate(30, tfm_y=TfmType.COORD),\n",
    "        RandomLighting(0.1,0.1, tfm_y=TfmType.COORD)]\n",
    "tfms = tfms_from_stats(imagenet_stats, sz, aug_tfms=augs, crop_type=CropType.NO, tfm_y=TfmType.COORD)\n",
    "trn_ds = CocoDS(train_df, train_path, sz, tfms[0])\n",
    "fix_ds = CocoDS(train_df, train_path, sz, tfms[1])\n",
    "# trn_dl = DataLoader(trn_ds, batch_size=bs)\n",
    "val_ds = CocoDS(val_df, val_path, sz, tfms[1])\n",
    "aug_ds = CocoDS(val_df, val_path, sz, tfms[0])\n",
    "ds = [trn_ds, val_ds, fix_ds, aug_ds, None, None]\n",
    "# val_dl = DataLoader(val_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ax = show_img(trn_ds.denorm(x)[i])\n",
    "im0_a = [k[i] for k in y]\n",
    "b = bb_hw(im0_a[0])\n",
    "draw_rect(ax, b)\n",
    "# draw_text(ax, b[:2], 'dog')\n",
    "draw_text(ax, b[:2], cat_map[str(lab2cat[im0_a[1].cpu().numpy().item()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=5\n",
    "fig,axes = plt.subplots(3,3, figsize=(9,9))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    x,y=next(iter(md.aug_dl))\n",
    "    ima=md.val_ds.denorm(to_np(x))[idx]\n",
    "    y1 = [k[idx] for k in y]\n",
    "    b = bb_hw(to_np(y1[0]))\n",
    "    print(b)\n",
    "    show_img(ima, ax=ax)\n",
    "    draw_rect(ax, b)\n",
    "    draw_text(ax, b[:2], cat_map[str(lab2cat[y1[1].cpu().numpy().item()])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = coco_path / 'dpath'\n",
    "data_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ImageData(data_path, ds, bs, num_workers=4, classes=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw2corners(ctr, hw): return torch.cat([ctr-hw/2,ctr+hw/2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9441, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na = 9\n",
    "anc_grids = [int(np.ceil(sz / 2**i)) for i in range(3, 8)]\n",
    "anc_zooms = [1., 2**(1/3), 2**(2/3)]\n",
    "anc_ratios = [(1.,1.), (1.,2), (2,1.)]\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "\n",
    "anc_x = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_y = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), na, axis=0)\n",
    "anc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "grid_sizes = V(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids]), requires_grad=False).unsqueeze(1)\n",
    "anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).float()\n",
    "anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])\n",
    "anchors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.reshape((to_np(anchor_cnr) + to_np(torch.randn(*anchor_cnr.size()))*0.01)*224, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.cm as cmx\n",
    "# import matplotlib.colors as mcolors\n",
    "# from cycler import cycler\n",
    "\n",
    "# def get_cmap(N):\n",
    "#     color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "#     return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n",
    "\n",
    "# num_colr = 12\n",
    "# cmap = get_cmap(num_colr)\n",
    "# colr_list = [cmap(float(x)) for x in range(num_colr)]\n",
    "\n",
    "# def show_ground_truth(ax, im, bbox, clas=None, prs=None, thresh=0.3):\n",
    "#     bb = [bb_hw(o) for o in bbox.reshape(-1,4)]\n",
    "#     if prs is None:  prs  = [None]*len(bb)\n",
    "#     if clas is None: clas = [None]*len(bb)\n",
    "#     ax = show_img(im, ax=ax)\n",
    "#     for i,(b,c,pr) in enumerate(zip(bb, clas, prs)):\n",
    "#         if((b[2]>0) and (pr is None or pr > thresh)):\n",
    "#             draw_rect(ax, b, color=colr_list[i%num_colr])\n",
    "#             txt = f'{i}: '\n",
    "#             if c is not None: txt += ('bg' if c==len(id2cat) else id2cat[c])\n",
    "#             if pr is not None: txt += f' {pr:.2f}'\n",
    "#             draw_text(ax, b[:2], txt, color=colr_list[i%num_colr])\n",
    "# fig, ax = plt.subplots(figsize=(7,7))\n",
    "# show_ground_truth(ax, x[0], a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50 = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=res50.layer2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.conv3.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res50_backbone = cut_model(res50, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res50_backbone = nn.Sequential(*cut_model(res50, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50_backbone = res50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tmp = ConvLearner.pretrained(resnet50, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tmp.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tmp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_out(k):\n",
    "    return (k-1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN_backbone(nn.Module):\n",
    "    def __init__(self, inch_list):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.backbone = backbone\n",
    "        \n",
    "        # expects c3, c4, c5 channel dims\n",
    "        self.inch_list = inch_list\n",
    "        self.feat_size = 256\n",
    "        self.p7_gen = nn.Conv2d(in_channels=self.feat_size, out_channels=self.feat_size, stride=2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.p6_gen = nn.Conv2d(in_channels=self.inch_list[2], \n",
    "                            out_channels=self.feat_size, kernel_size=3, stride=2, padding=pad_out(3))\n",
    "        self.p5_gen1 = nn.Conv2d(in_channels=self.inch_list[2], \n",
    "                                 out_channels=self.feat_size, kernel_size=1, padding=pad_out(1))\n",
    "        self.p5_gen2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.p5_gen3 = nn.Conv2d(in_channels=self.feat_size, out_channels=self.feat_size,\n",
    "                                kernel_size=3, padding=pad_out(3))\n",
    "        \n",
    "        self.p4_gen1 = nn.Conv2d(in_channels=self.inch_list[1], out_channels=self.feat_size, kernel_size=1,\n",
    "                                padding=pad_out(1))\n",
    "        self.p4_gen2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.p4_gen3 = nn.Conv2d(in_channels=self.feat_size, out_channels=self.feat_size, kernel_size=3, \n",
    "                                padding=pad_out(3))\n",
    "        \n",
    "        self.p3_gen1 = nn.Conv2d(in_channels=self.inch_list[0], out_channels=self.feat_size, kernel_size=1,\n",
    "                                padding=pad_out(1))\n",
    "        self.p3_gen2 = nn.Conv2d(in_channels=self.feat_size, out_channels=self.feat_size, kernel_size=3,\n",
    "                                padding=pad_out(3))\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # expects inp to be output of c3, c4, c5\n",
    "        c3 = inp[0]\n",
    "        c4 = inp[1]\n",
    "        c5 = inp[2]\n",
    "        p51 = self.p5_gen1(c5)\n",
    "        p5_out = self.p5_gen3(p51)\n",
    "        \n",
    "        p5_up = self.p5_gen2(p51)\n",
    "        p41 = self.p4_gen1(c4) + p5_up\n",
    "        p4_out = self.p4_gen3(p41)\n",
    "        \n",
    "        p4_up = self.p4_gen2(p41)\n",
    "        p31 = self.p3_gen1(c3) + p4_up\n",
    "        p3_out = self.p3_gen2(p31)\n",
    "        \n",
    "        p6_out = self.p6_gen(c5)\n",
    "        \n",
    "        p7_out = self.p7_gen(F.relu(p6_out))\n",
    "        \n",
    "        return [p3_out, p4_out, p5_out, p6_out, p7_out]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vals(mdl):\n",
    "    for m in mdl.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classf_model(nn.Module):\n",
    "    def __init__(self, fs=256, na=9, nc=80):\n",
    "        self.na = na\n",
    "        self.nc = nc\n",
    "        self.feat_size = fss\n",
    "        self.classification_model = nn.Sequential(nn.ModuleList([nn.Conv2d(in_channels=self.feat_size,\n",
    "                                                                           out_channels=self.feat_size,\n",
    "                                                                           kernel_size=3, padding=1)]*4),\n",
    "                                                  nn.Conv2d(in_channels=self.feat_size,\n",
    "                                                            out_channels=self.na * self.nc,\n",
    "                                                            kernel_size=3, padding=1))\n",
    "        initialize_vals(self.classification_model)\n",
    "    def forward(self, inp):\n",
    "        out = self.classification_model(inp)\n",
    "        out2 = flatten_conv(out, self.na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regress_model(nn.Module):\n",
    "    def __init__(self, fs=256, na=9, nc=80):\n",
    "        self.na = na\n",
    "        self.nc = nc\n",
    "        self.feat_size = fss\n",
    "        self.reg_model = nn.Sequential(nn.ModuleList([nn.Conv2d(in_channels=self.feat_size,\n",
    "                                                                           out_channels=self.feat_size,\n",
    "                                                                           kernel_size=3, padding=1)]*4),\n",
    "                                                  nn.Conv2d(in_channels=self.feat_size,\n",
    "                                                            out_channels=self.na * 4,\n",
    "                                                            kernel_size=3, padding=1))\n",
    "        initialize_vals(self.regress_model)\n",
    "    def forward(self, inp):\n",
    "        out = self.reg_model(inp)\n",
    "        out2 = flatten_conv(out, self.na)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class retina_net_model(nn.Module):\n",
    "    def __init__(self, resnet_model):\n",
    "        super().__init__()\n",
    "        self.res_backbone = resnet_model\n",
    "        self.fpn_sizes = [self.res_backbone.layer2[-1].conv3.out_channels, \n",
    "                          self.res_backbone.layer3[-1].conv3.out_channels,\n",
    "                          self.res_backbone.layer4[-1].conv3.out_channels]\n",
    "        self.feat_size = 256\n",
    "        self.num_anch = 9\n",
    "        self.num_class = 80\n",
    "        self.fpn = FPN_backbone(self.fpn_sizes)\n",
    "        self.cls_model = classf_model(self.fs, self.num_anch, self.num_class)\n",
    "        self.reg_model = regress_model(self.fs, self.num_anch, self.num_class)\n",
    "        \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        features = self.fpn([x2, x3, x4])\n",
    "        out_cls = []\n",
    "        out_bbx = []\n",
    "        for p in features:\n",
    "            out_cls.append(self.cls_model(p))\n",
    "            out_bbx.append(self.reg_model(p))\n",
    "        \n",
    "        return [torch.cat(out_cls, dim=1),\n",
    "                torch.cat(out_box, dim=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
