{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.common.util import JsonDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DatasetReader.register(\"s2_papers\")\n",
    "class SemanticScholarDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a JSON-lines file containing papers from the Semantic Scholar database, and creates a\n",
    "    dataset suitable for document classification using these papers.\n",
    "    Expected format for each input line: {\"paperAbstract\": \"text\", \"title\": \"text\", \"venue\": \"text\"}\n",
    "    The JSON could have other fields, too, but they are ignored.\n",
    "    The output of ``read`` is a list of ``Instance`` s with the fields:\n",
    "        title: ``TextField``\n",
    "        abstract: ``TextField``\n",
    "        label: ``LabelField``\n",
    "    where the ``label`` is derived from the venue of the paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lazy : ``bool`` (optional, default=False)\n",
    "        Passed to ``DatasetReader``.  If this is ``True``, training will start sooner, but will\n",
    "        take longer per batch.  This also allows training with datasets that are too large to fit\n",
    "        in memory.\n",
    "    tokenizer : ``Tokenizer``, optional\n",
    "        Tokenizer to use to split the title and abstrct into words or other kinds of tokens.\n",
    "        Defaults to ``WordTokenizer()``.\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        Indexers used to define input token representations. Defaults to ``{\"tokens\":\n",
    "        SingleIdTokenIndexer()}``.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 lazy: bool = False,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "            for line in data_file:\n",
    "                line = line.strip(\"\\n\")\n",
    "                if not line:\n",
    "                    continue\n",
    "                paper_json = json.loads(line)\n",
    "                title = paper_json['title']\n",
    "                abstract = paper_json['paperAbstract']\n",
    "                venue = paper_json['venue']\n",
    "                yield self.text_to_instance(title, abstract, venue)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, title: str, abstract: str, venue: str = None) -> Instance:  # type: ignore\n",
    "        # pylint: disable=arguments-differ\n",
    "        tokenized_title = self._tokenizer.tokenize(title)\n",
    "        tokenized_abstract = self._tokenizer.tokenize(abstract)\n",
    "        title_field = TextField(tokenized_title, self._token_indexers)\n",
    "        abstract_field = TextField(tokenized_abstract, self._token_indexers)\n",
    "        fields = {'title': title_field, 'abstract': abstract_field}\n",
    "        if venue is not None:\n",
    "            fields['label'] = LabelField(venue)\n",
    "        return Instance(fields)\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params: Params) -> 'SemanticScholarDatasetReader':\n",
    "        lazy = params.pop('lazy', False)\n",
    "        tokenizer = Tokenizer.from_params(params.pop('tokenizer', {}))\n",
    "        token_indexers = TokenIndexer.dict_from_params(params.pop('token_indexers', {}))\n",
    "        params.assert_empty(cls.__name__)\n",
    "        return cls(lazy=lazy, tokenizer=tokenizer, token_indexers=token_indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"paper_classifier\")\n",
    "class AcademicPaperClassifier(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` performs text classification for an academic paper.  We assume we're given a\n",
    "    title and an abstract, and we predict some output label.\n",
    "    The basic model structure: we'll embed the title and the abstract, and encode each of them with\n",
    "    separate Seq2VecEncoders, getting a single vector representing the content of each.  We'll then\n",
    "    concatenate those two vectors, and pass the result through a feedforward network, the output of\n",
    "    which we'll use as our scores for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``, required\n",
    "        A Vocabulary, required in order to compute sizes for input/output projections.\n",
    "    text_field_embedder : ``TextFieldEmbedder``, required\n",
    "        Used to embed the ``tokens`` ``TextField`` we get as input to the model.\n",
    "    title_encoder : ``Seq2VecEncoder``\n",
    "        The encoder that we will use to convert the title to a vector.\n",
    "    abstract_encoder : ``Seq2VecEncoder``\n",
    "        The encoder that we will use to convert the abstract to a vector.\n",
    "    classifier_feedforward : ``FeedForward``\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 title_encoder: Seq2VecEncoder,\n",
    "                 abstract_encoder: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.title_encoder = title_encoder\n",
    "        self.abstract_encoder = abstract_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        if text_field_embedder.get_output_dim() != title_encoder.get_input_dim():\n",
    "            raise ConfigurationError(\"The output dimension of the text_field_embedder must match the \"\n",
    "                                     \"input dimension of the title_encoder. Found {} and {}, \"\n",
    "                                     \"respectively.\".format(text_field_embedder.get_output_dim(),\n",
    "                                                            title_encoder.get_input_dim()))\n",
    "        if text_field_embedder.get_output_dim() != abstract_encoder.get_input_dim():\n",
    "            raise ConfigurationError(\"The output dimension of the text_field_embedder must match the \"\n",
    "                                     \"input dimension of the abstract_encoder. Found {} and {}, \"\n",
    "                                     \"respectively.\".format(text_field_embedder.get_output_dim(),\n",
    "                                                            abstract_encoder.get_input_dim()))\n",
    "        self.metrics = {\n",
    "                \"accuracy\": CategoricalAccuracy(),\n",
    "                \"accuracy3\": CategoricalAccuracy(top_k=3)\n",
    "        }\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                title: Dict[str, torch.LongTensor],\n",
    "                abstract: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        title : Dict[str, Variable], required\n",
    "            The output of ``TextField.as_array()``.\n",
    "        abstract : Dict[str, Variable], required\n",
    "            The output of ``TextField.as_array()``.\n",
    "        label : Variable, optional (default = None)\n",
    "            A variable representing the label for each instance in the batch.\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        class_probabilities : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_classes)`` representing a distribution over the\n",
    "            label classes for each instance.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "        embedded_title = self.text_field_embedder(title)\n",
    "        title_mask = util.get_text_field_mask(title)\n",
    "        encoded_title = self.title_encoder(embedded_title, title_mask)\n",
    "\n",
    "        embedded_abstract = self.text_field_embedder(abstract)\n",
    "        abstract_mask = util.get_text_field_mask(abstract)\n",
    "        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)\n",
    "\n",
    "        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))\n",
    "        output_dict = {'logits': logits}\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label.squeeze(-1))\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label.squeeze(-1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Does a simple argmax over the class probabilities, converts indices to string labels, and\n",
    "        adds a ``\"label\"`` key to the dictionary with the result.\n",
    "        \"\"\"\n",
    "        class_probabilities = F.softmax(output_dict['logits'], dim=-1)\n",
    "        output_dict['class_probabilities'] = class_probabilities\n",
    "\n",
    "        predictions = class_probabilities.cpu().data.numpy()\n",
    "        argmax_indices = numpy.argmax(predictions, axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n",
    "                  for x in argmax_indices]\n",
    "        output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'AcademicPaperClassifier':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n",
    "        title_encoder = Seq2VecEncoder.from_params(params.pop(\"title_encoder\"))\n",
    "        abstract_encoder = Seq2VecEncoder.from_params(params.pop(\"abstract_encoder\"))\n",
    "        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n",
    "\n",
    "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
    "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
    "\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   title_encoder=title_encoder,\n",
    "                   abstract_encoder=abstract_encoder,\n",
    "                   classifier_feedforward=classifier_feedforward,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Predictor.register('paper-classifier')\n",
    "class PaperClassifierPredictor(Predictor):\n",
    "    \"\"\"\"Predictor wrapper for the AcademicPaperClassifier\"\"\"\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Tuple[Instance, JsonDict]:\n",
    "        title = json_dict['title']\n",
    "        abstract = json_dict['paperAbstract']\n",
    "        instance = self._dataset_reader.text_to_instance(title=title, abstract=abstract)\n",
    "\n",
    "        # label_dict will be like {0: \"ACL\", 1: \"AI\", ...}\n",
    "        label_dict = self._model.vocab.get_index_to_token_vocabulary('labels')\n",
    "        # Convert it to list [\"ACL\", \"AI\", ...]\n",
    "        all_labels = [label_dict[i] for i in range(len(label_dict))]\n",
    "\n",
    "        return instance, {\"all_labels\": all_labels}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
