{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is the place where you instantiate all your modules\n",
    "        # you can later access them using the same names you've given them in\n",
    "        # here\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "        self.sqn = nn.Sequential(nn.Conv2d(1, 10, 5), nn.ReLU(),nn.MaxPool2d(2, 2),\n",
    "                                nn.Conv2d(10, 20, 5), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "                                Flatten(), nn.Linear(320, 50), nn.ReLU(), nn.Linear(50, 10),\n",
    "                                nn.ReLU())\n",
    "\n",
    "    # it's the forward function that defines the network structure\n",
    "    # we're accepting only a single input in here, but if you want,\n",
    "    # feel free to use more\n",
    "    def forward(self, input):\n",
    "#         x = self.pool1(F.relu(self.conv1(input)))\n",
    "#         x = self.pool2(F.relu(self.conv2(x)))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.sqn(input)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (sqn): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    (6): Flatten(\n",
      "    )\n",
      "    (7): Linear(in_features=320, out_features=50, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (10): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MNISTConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv2.register_backward_hook(printgradnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = to_gpu(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = V(torch.LongTensor([3]))\n",
    "loss_fn = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d backward\n",
      "Inside class:Conv2d\n",
      "\n",
      "grad_input:  <class 'tuple'>\n",
      "grad_input[0]:  <class 'torch.autograd.variable.Variable'>\n",
      "grad_output:  <class 'tuple'>\n",
      "grad_output[0]:  <class 'torch.autograd.variable.Variable'>\n",
      "\n",
      "grad_input size: torch.Size([1, 10, 12, 12])\n",
      "grad_output size: torch.Size([1, 20, 8, 8])\n",
      "grad_input norm: Variable containing:\n",
      " 0.1056\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = net(V(inp))\n",
    "err = loss_fn(out, target)\n",
    "err.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.FloatTensor(1, out.size()[-1]).zero_()\n",
    "one_hot[0][0] = 1.0\n",
    "one_hot = to_gpu(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d backward\n",
      "Inside class:Conv2d\n",
      "\n",
      "grad_input:  <class 'tuple'>\n",
      "grad_input[0]:  <class 'torch.autograd.variable.Variable'>\n",
      "grad_output:  <class 'tuple'>\n",
      "grad_output[0]:  <class 'torch.autograd.variable.Variable'>\n",
      "\n",
      "grad_input size: torch.Size([1, 10, 12, 12])\n",
      "grad_output size: torch.Size([1, 20, 8, 8])\n",
      "grad_input norm: Variable containing:\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward(gradient=one_hot, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_list = list(net.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', MNISTConvNet(\n",
       "    (sqn): Sequential(\n",
       "      (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (4): ReLU()\n",
       "      (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "      (6): Flatten(\n",
       "      )\n",
       "      (7): Linear(in_features=320, out_features=50, bias=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=50, out_features=10, bias=True)\n",
       "      (10): ReLU()\n",
       "    )\n",
       "  )), ('sqn', Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (6): Flatten(\n",
       "    )\n",
       "    (7): Linear(in_features=320, out_features=50, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (10): ReLU()\n",
       "  )), ('sqn.0', Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))), ('sqn.1',\n",
       "  ReLU()), ('sqn.2',\n",
       "  MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)), ('sqn.3',\n",
       "  Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))), ('sqn.4',\n",
       "  ReLU()), ('sqn.5', MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)), ('sqn.6',\n",
       "  Flatten(\n",
       "  )), ('sqn.7', Linear(in_features=320, out_features=50, bias=True)), ('sqn.8',\n",
       "  ReLU()), ('sqn.9',\n",
       "  Linear(in_features=50, out_features=10, bias=True)), ('sqn.10', ReLU())]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f3e47caa208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_list[-8][1].register_backward_hook(printgradnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
